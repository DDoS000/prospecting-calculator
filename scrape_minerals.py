import requests
from bs4 import BeautifulSoup
import json
import time

BASE_URL = "https://prospecting.fandom.com"
MINERALS_URL = BASE_URL + "/wiki/Minerals"


def get_soup(url):
    res = requests.get(url)
    res.raise_for_status()
    return BeautifulSoup(res.text, "html.parser")


def scrape_locations(mineral_url):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Locations & Chances ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡πÅ‡∏£‡πà"""
    soup = get_soup(mineral_url)
    locations = []
    drop_chances = {}

    # ‡∏´‡∏≤ section ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Locations
    headers = soup.find_all(["h2", "h3"])
    for h in headers:
        if "Locations" in h.get_text():
            # ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á heading ‡∏ô‡∏µ‡πâ
            next_element = h.find_next_sibling()
            while next_element:
                if next_element.name == "ul":
                    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å list
                    for li in next_element.find_all("li"):
                        text = li.get_text(strip=True)
                        # ‡πÅ‡∏¢‡∏Å location ‡πÅ‡∏•‡∏∞ chance (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                        if "(" in text and "%" in text:
                            parts = text.split("(")
                            loc = parts[0].strip()
                            chance = "(" + parts[1] if len(parts) > 1 else ""
                            locations.append(loc)
                            drop_chances[loc] = chance
                        else:
                            locations.append(text)
                    break
                elif next_element.name in ["h2", "h3"]:
                    break
                next_element = next_element.find_next_sibling()
            break

    # ‡∏´‡∏≤‡∏à‡∏≤‡∏Å infobox ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠
    if not locations:
        infobox = soup.find("table", {"class": "infobox"})
        if infobox:
            rows = infobox.find_all("tr")
            for row in rows:
                th = row.find("th")
                if th and "Locations" in th.get_text():
                    td = row.find("td")
                    if td:
                        for li in td.find_all("li"):
                            text = li.get_text(strip=True)
                            locations.append(text)

    return locations, drop_chances


def scrape_minerals():
    soup = get_soup(MINERALS_URL)
    minerals = []

    # ‡∏´‡∏≤‡πÅ‡∏ó‡πá‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ rarity ‡∏à‡∏≤‡∏Å tabber
    tabber = soup.find("div", {"class": "tabber wds-tabber"})
    if not tabber:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö tabber element")
        return minerals

    # ‡∏´‡∏≤ tab content ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    tab_contents = tabber.find_all("div", {"class": "wds-tab__content"})
    tab_labels = tabber.find_all("li", {"class": "wds-tabs__tab"})
    
    for i, tab_content in enumerate(tab_contents):
        # ‡∏´‡∏≤ rarity ‡∏à‡∏≤‡∏Å tab label
        rarity = "Unknown"
        if i < len(tab_labels):
            rarity_elem = tab_labels[i].find("a")
            if rarity_elem:
                rarity = rarity_elem.get_text(strip=True)

        print(f"üîç Processing {rarity} minerals...")

        # ‡∏´‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ tab
        table = tab_content.find("table", {"class": "fandom-table"})
        if not table:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô {rarity} tab")
            continue

        rows = table.find_all("tr")[1:]  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á

        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 5:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                continue

            # Columns: [Name, Appearance, $/kg, Collection description, Available Locations]
            name_cell = cols[0]
            value_cell = cols[2]
            description_cell = cols[3]
            locations_cell = cols[4]

            # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞ URL
            link = name_cell.find("a")
            name = link.text.strip() if link else name_cell.get_text(strip=True)
            url = BASE_URL + link["href"] if link else None

            # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤
            value = value_cell.get_text(strip=True)

            # ‡∏î‡∏∂‡∏á description
            description = description_cell.get_text(strip=True)

            # ‡∏î‡∏∂‡∏á locations ‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á
            table_locations = []
            for loc_link in locations_cell.find_all("a"):
                table_locations.append(loc_link.get_text(strip=True))
            
            if not table_locations:
                table_locations = [locations_cell.get_text(strip=True)]

            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡πÅ‡∏£‡πà
            detailed_locations, drop_chances = ([], {})
            if url:
                try:
                    detailed_locations, drop_chances = scrape_locations(url)
                    time.sleep(0.5)  # ‡∏Å‡∏±‡∏ô block
                except Exception as e:
                    print(f"‚ö†Ô∏è Error scraping {name}: {e}")

            # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡πà‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á
            final_locations = detailed_locations if detailed_locations else table_locations

            mineral = {
                "name": name,
                "rarity": rarity,
                "value": value,
                "description": description,
                "locations": final_locations,
                "dropChances": drop_chances,
                "url": url
            }
            print(f'‚úÖ Got data for {mineral}')
            minerals.append(mineral)

    return minerals


if __name__ == "__main__":
    minerals = scrape_minerals()
    with open("minerals.json", "w", encoding="utf-8") as f:
        json.dump(minerals, f, indent=2, ensure_ascii=False)
    print(json.dumps(minerals, indent=2, ensure_ascii=False))
